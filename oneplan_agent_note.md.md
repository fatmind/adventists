在构建通用任务助理的过程中，当前的 “走一步看一步” (ReAct) 模式正面临响应慢、成本高和长链路出错的挑战。本次内部分享了“H”如何通过 **OnePlan（一次性规划）** 与 **异步上下文管理**，让 30B 规模的模型在复杂任务中展现出超越 235B 模型的执行能力。

## 一、 OnePlan：从 “走一步看一步” 到 “全局plan”

### 1. 核心痛点：为什么 ReAct 不够用了？
传统的 Planning + ReAct 路径就像一个没有地图的旅行者，每走一公里都要停下来问路。
*   **误差累积：** 随着对话增加，上下文变得臃肿。实验表明，在执行长任务时，后期 “思维链决策” 受历史干扰严重，会导致模型“晕头转向”。
*   **资源冗余：** 每一轮工具调用都要请求一次 LLM，不仅耗费 Token，更让用户在屏幕前陷入漫长的等待。

### 2. 解决方案：一次性 Plan 的“上帝视角”
OnePlan 的核心逻辑是：**在初始最“干净”的上下文中，直接规划出完整的工具执行轨迹**。
*   **降维打击：** 通过这种架构，30B 模型在真实流量测试中，初始规划准确率达 **97%**，重推理准确率达 **95%**，水位持平 235B 超大模型。从侧面说明这种架构，更容易微调模型，在 2C 场景追求 RT 必须上小参数。
*   **减少交互：** 因为减少了模型请求次数，总体 Token 消耗反而降低了。

### 3. 技术原理与深度细节
OnePlan 在首次推理时会输出一个包含 `action_plan` 的 JSON，定义好工具的先后顺序与依赖关系。

#### A. 工具依赖
**工具依赖 (Dependence)：** 当下一步参数依赖上一步结果时，会在计划中明确标注。

**代码示例：** 和下面 锚点工具 差别是，这个工具是确定的，只是不知道输入，下面是选哪个工具也不知道

```json
{
  "action_plan": {
    "step1": { "function": "search_city", "args": {"query": "春晚分会场"} },
    "step2": { 
      "function": "book_flight", 
      "args": {"to": "$$step1.result$$"}, 
      "dependence": [1] // 明确依赖第一步结果
    }
  }
}
```

#### B. 锚点工具 与 Replan 触发

锚点工具本质上是为**逻辑分支**预留的占位符。当模型预测到某一步必须依赖前面的具体“结论”才能做决定时，就会放入锚点。

**代码示例：条件分支场景**
任务：“帮我查一下杭州今天下午 6 点的天气，如果晴朗就订西湖餐厅，下雨就点黄焖鸡外卖。”

**执行逻辑：** 系统执行完 `step1` 获取天气后，由于 `step2` 是 `anchor_function`，它会强制触发一次 **Replan**（重推理）。此时模型会根据“天气已查到，是下雨”这个新事实，将计划动态更新为调用外卖工具。

```json
{
  "action_plan": {
    "step1": {
      "function": "search_weather",
      "args": {"city": "杭州", "time": "18:00"},
      "dependence": []
    },
    "step2": {
      "function": "anchor_function",
      "args": {
        "task": "根据step1结果：若晴朗则调用restaurant_booking，若下雨则调用food_delivery"
      },
      "dependence": [1] // 明确依赖第一步的天气数据
    }
  }
}
```
**触发 Replan：** 只有当遇到“锚点”、工具执行失败（断网/报错）或工具返回 `status=fail` 时，系统才会执行 **Replan**（重推理）。这是一种“被动触发”机制，最大程度保证了主链路的顺滑。

### 2. 适用场景与边界：什么时候该用，什么时候不灵？

**a、适用场景（Where it works）**

*   **确定性的逻辑分叉：** 任务相对有明确的“如果...就...”内在逻辑，且分支路径是已知的工具组合（如：订票、查天气、发邮件）

**b、不适用场景（The Boundaries）**

*   **极度不确定的开放式探索：** 如果任务每一步都完全不可预知（例如：在未知领域进行开放式科研），“一次性规划”就会失效，系统会退化为传统的步步 ReAct 模式。

**c、并非所有工具都要 “重推理”**

*   **无需重推理的场景：** 部分工具，像“写文章”类的工具，其内部本身就能看到全局上下文，可以直接在工具内部完成复杂逻辑。



## 二、 上下文管理：不是简单 “暴力压缩”

### 1. 核心痛点
Agent 执行长任务时产生的大量工具结果（如网页全文、代码日志）会迅速挤占 Token 配额。这不仅会导致 **Token 成本激增**，还会触发模型在长上下文下的 **“性能腐败”（Context Rot）**，使其难以从臃肿的记忆中提取关键信息，产生幻觉或忽视核心指令。

### 2. 解决方案：STM/LTM 分层与异步处理
**“零感知”体验：** 压缩过程在主链路执行过程中同步异步完成，用户完全感知不到压缩带来的延迟：

*   **短期记忆 (STM)：** 保留“上一步”工具调用的原始结果，确保当前推理的极高精度。
*   **长期记忆 (LTM)：** 对更早的历史信息进行摘要或裁剪，且压缩过程与主 Agent 的执行**异步并行**，不增加用户的等待时延。

### 3. 技术原理：STM/LTM 分层与并行处理
“代号H”将记忆分为**短期记忆 (STM)** 和**长期记忆 (LTM)**。

*   **STM（短期记忆）：** 指**上一步**工具调用的原始结果。它对当前推理至关重要，因此完整保留。
*   **LTM（长期记忆）：** 指早于上一步的历史信息。

**异步处理逻辑：**
1.  **Phase 1-1：** 主 Agent 执行当前工具 $T_s$。
2.  **Phase 1-2（异步）：** 后台系统同步处理前两个工具的结果，更新长期记忆 $L_{s-2}$。如果 $L$ 的 Token 长度超过警戒阈值，立即触发**压缩模块**，用摘要替换原始记忆。
3.  **Phase 2：** 当下一轮循环开始时，主 Agent 的上下文 $M_s$ 已自动更新为：
    $M_s = \{L_{s-2}, f(T_{s-1})\}$（其中 $L_{s-2}$ 为压缩后的长记忆，$f(T_{s-1})$ 为上一步原始结果）。
4.   原始工具内容会被独立存储，确保可回溯

#### 基于场景的精细化设计

代号H 并非简单的“满了就删”，而是根据任务特性采取不同的策略：

*   **垂直场景的保留机制：**
    例如在 **“做 PPT”** 任务中，系统会确保存储原始工具内容。因为 PPT 的生成对信息的完整性和排版细节要求极高，主 Agent 看到的虽然是摘要，但特定工具在执行时仍能 回溯 原始内容。
    
*   **子 Agent 过滤冗余：**
    在处理搜索任务时，引入 **search 子 Agent**，但注意此时不传递 “上下文”，这个 子 agent 作用是一个 “计算器” 工具。
    
    子 Agent 在其独立的上下文空间内消化掉成百上千条原始信息，主链路得以保持“轻量化”。
    
    ```json
    // 主 Agent 只接收 search 过滤后的精炼结论
    {
      "step": "search",
      "args": {"query": "杭州周末天气及茶馆建议"},
      "result": "周六晴, 建议去青藤茶馆; 周日雨, 建议室内体验。 [此处省略了数百条原始搜索结果]"
    }
    ```

### 4. 面向任务特性的“压缩” vs “卸载”

结合最新导入的 Manus 等框架资料，上下文管理的底层哲学正在发生分化：

*   **压缩模式（代号H 方案）：**
    
    *   **核心逻辑：** 总结并丢弃。利用模型生成摘要来替代原文。
    *   **适用场景：** 线性、目标明确的 2C 任务。**关键：高度抽象看，任务是线性的、或者叫流程式的**
    *   **优点：** 极大节省 Token，让中小规模模型（如 30B）也能在有限窗口内完成复杂流程。
    *   **风险：** **不可逆的信息损失**。如果模型在第 10 步压缩掉了某个细节，而该细节在第 100 步突然变得关键，Agent 将陷入死循环或幻觉。如果任务是线性的，损失可能性就很小，因为前一个任务的执行细节、对后一步是无意义的。
    
*   **卸载模式（Manus 方案）：**
    * **核心逻辑：** 存储并索引。Manus 将文件系统视为“无限的上下文”，直接把长文本以文件形式存入沙箱（Offloading），主窗口只留一个 **“文件路径指针”**。
    
    * **适用场景：** 研发、深度调研等对“低熵”信息（如错误码、精确坐标）极度敏感的任务。
    
    * **优点：** **可逆的紧凑化（Reversible Compaction）**。模型随时可以用 `grep` 等工具重新找回“被遗忘”的细节。
    
    * 风险：仍然存在上下文腐化，在一些特定场景，如 大量数据 走 代码处理 只返回结果、compact 时摸索特定 schema 确保关键信息保留。核心还是对 “低熵” 信息的容忍度有多高。
    
      